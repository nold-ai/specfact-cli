"""
End-to-end tests for plan review and batch updates (interactive and non-interactive modes).

This test suite verifies:
- Interactive mode: Selective updates via prompts
- Non-interactive mode: Batch updates via file upload
- Batch updates for features via file
- Batch updates for stories via file
- List findings in different formats (JSON, YAML, table)
"""

from __future__ import annotations

import json
from pathlib import Path
from unittest.mock import patch

import pytest
import yaml
from typer.testing import CliRunner

from specfact_cli.cli import app
from specfact_cli.models.plan import Feature, Idea, Metadata, PlanBundle, Product, Story


runner = CliRunner()


@pytest.fixture
def workspace(tmp_path: Path) -> Path:
    """Create a temporary workspace with .specfact structure."""
    workspace = tmp_path / "batch_updates_workspace"
    workspace.mkdir()
    (workspace / ".specfact").mkdir()
    (workspace / ".specfact" / "plans").mkdir()
    return workspace


@pytest.fixture
def incomplete_plan(workspace: Path) -> Path:
    """Create an incomplete plan bundle for testing."""
    plan_path = workspace / ".specfact" / "plans" / "test-plan.bundle.yaml"

    bundle = PlanBundle(
        version="1.0",
        idea=Idea(
            title="Test Plan",
            narrative="",  # Empty narrative - will trigger question
            target_users=[],
            value_hypothesis="",
            constraints=[],
            metrics=None,
        ),
        business=None,
        product=Product(themes=["Core"], releases=[]),
        features=[
            Feature(
                key="FEATURE-001",
                title="Incomplete Feature",
                outcomes=[],
                acceptance=[],  # Missing acceptance criteria
                constraints=[],
                stories=[
                    Story(
                        key="STORY-001",
                        title="Generic task",
                        acceptance=[],  # Empty acceptance - will trigger question
                        story_points=0,
                        value_points=0,
                        confidence=0.5,
                        draft=False,
                        scenarios=None,
                        contracts=None,
                    )
                ],
                confidence=0.8,
                draft=False,
            ),
            Feature(
                key="FEATURE-002",
                title="Another Incomplete Feature",
                outcomes=[],
                acceptance=[],
                constraints=[],
                stories=[],
                confidence=0.7,
                draft=False,
            ),
        ],
        metadata=Metadata(
            stage="draft",
            promoted_at=None,
            promoted_by=None,
            analysis_scope=None,
            entry_point=None,
            external_dependencies=[],
            summary=None,
        ),
        clarifications=None,
    )

    with plan_path.open("w") as f:
        yaml.dump(bundle.model_dump(), f, default_flow_style=False)

    return plan_path


class TestListFindingsOutput:
    """Test --list-findings option with different output formats."""

    def test_list_findings_json_format(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test --list-findings outputs valid JSON."""
        monkeypatch.chdir(workspace)

        result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--findings-format",
                "json",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Parse JSON output
        output_lines = result.stdout.strip().split("\n")
        json_start = None
        for i, line in enumerate(output_lines):
            if line.strip().startswith("{"):
                json_start = i
                break

        assert json_start is not None, "No JSON found in output"

        json_str = "\n".join(output_lines[json_start:])
        data = json.loads(json_str)

        # Validate structure
        assert "findings" in data
        assert "coverage" in data
        assert "total_findings" in data
        assert "priority_score" in data
        assert isinstance(data["findings"], list)
        assert isinstance(data["coverage"], dict)
        assert isinstance(data["total_findings"], int)
        assert isinstance(data["priority_score"], (int, float))

        # Validate finding structure
        if len(data["findings"]) > 0:
            finding = data["findings"][0]
            assert "category" in finding
            assert "status" in finding
            assert "description" in finding
            assert "impact" in finding
            assert "uncertainty" in finding
            assert "priority" in finding
            assert "question" in finding
            assert "related_sections" in finding

    def test_list_findings_yaml_format(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test --list-findings outputs valid YAML."""
        monkeypatch.chdir(workspace)

        result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--findings-format",
                "yaml",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Parse YAML output
        output_lines = result.stdout.strip().split("\n")
        yaml_start = None
        for i, line in enumerate(output_lines):
            if line.strip().startswith("findings:") or line.strip().startswith("coverage:"):
                yaml_start = i
                break

        assert yaml_start is not None, "No YAML found in output"

        yaml_str = "\n".join(output_lines[yaml_start:])
        data = yaml.safe_load(yaml_str)

        # Validate structure
        assert "findings" in data
        assert "coverage" in data
        assert "total_findings" in data
        assert "priority_score" in data

    def test_list_findings_table_format(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test --list-findings outputs table (interactive mode)."""
        monkeypatch.chdir(workspace)

        result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--findings-format",
                "table",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0
        # Table output should contain headers (may be truncated in table)
        assert "Category" in result.stdout or "category" in result.stdout.lower()
        assert "Status" in result.stdout or "status" in result.stdout.lower()
        # Description may be truncated as "Descriâ€¦" in table output
        assert (
            "Descri" in result.stdout
            or "descri" in result.stdout.lower()
            or "Description" in result.stdout
            or "description" in result.stdout.lower()
        )

    def test_list_findings_default_format_non_interactive(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test --list-findings defaults to JSON in non-interactive mode."""
        monkeypatch.chdir(workspace)

        result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--non-interactive",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Should output JSON (default for non-interactive)
        output_lines = result.stdout.strip().split("\n")
        json_start = None
        for i, line in enumerate(output_lines):
            if line.strip().startswith("{"):
                json_start = i
                break

        assert json_start is not None, "Should output JSON in non-interactive mode"

    def test_list_findings_default_format_interactive(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test --list-findings defaults to table in interactive mode."""
        monkeypatch.chdir(workspace)

        result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0
        # Should output table (default for interactive)
        assert "Category" in result.stdout or "category" in result.stdout.lower()


class TestBatchFeatureUpdates:
    """Test batch updates for features via file upload."""

    def test_batch_update_features_from_file(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test updating multiple features via batch file."""
        monkeypatch.chdir(workspace)

        # Create batch update file
        updates_file = workspace / "feature_updates.json"
        updates = [
            {
                "key": "FEATURE-001",
                "title": "Updated Feature 1",
                "outcomes": ["Outcome 1", "Outcome 2"],
                "acceptance": ["Acceptance 1", "Acceptance 2"],
                "confidence": 0.9,
            },
            {
                "key": "FEATURE-002",
                "title": "Updated Feature 2",
                "outcomes": ["Outcome 3"],
                "acceptance": ["Acceptance 3"],
                "confidence": 0.85,
            },
        ]
        updates_file.write_text(json.dumps(updates, indent=2))

        result = runner.invoke(
            app,
            [
                "plan",
                "update-feature",
                "--batch-updates",
                str(updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Verify updates were applied
        with incomplete_plan.open() as f:
            updated_bundle_data = yaml.safe_load(f)
            updated_bundle = PlanBundle(**updated_bundle_data)

        # Find updated features
        feature_1 = next((f for f in updated_bundle.features if f.key == "FEATURE-001"), None)
        feature_2 = next((f for f in updated_bundle.features if f.key == "FEATURE-002"), None)

        assert feature_1 is not None
        assert feature_1.title == "Updated Feature 1"
        assert feature_1.outcomes == ["Outcome 1", "Outcome 2"]
        assert feature_1.acceptance == ["Acceptance 1", "Acceptance 2"]
        assert feature_1.confidence == 0.9

        assert feature_2 is not None
        assert feature_2.title == "Updated Feature 2"
        assert feature_2.outcomes == ["Outcome 3"]
        assert feature_2.acceptance == ["Acceptance 3"]
        assert feature_2.confidence == 0.85

    def test_batch_update_features_partial_updates(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test batch updates with partial feature data (only some fields)."""
        monkeypatch.chdir(workspace)

        # Create batch update file with partial updates
        updates_file = workspace / "partial_updates.json"
        updates = [
            {
                "key": "FEATURE-001",
                "confidence": 0.95,  # Only update confidence
            },
            {
                "key": "FEATURE-002",
                "title": "New Title",  # Only update title
            },
        ]
        updates_file.write_text(json.dumps(updates, indent=2))

        # Read original plan
        with incomplete_plan.open() as f:
            original_bundle_data = yaml.safe_load(f)
            original_bundle = PlanBundle(**original_bundle_data)

        original_feature_1 = next((f for f in original_bundle.features if f.key == "FEATURE-001"), None)
        original_feature_2 = next((f for f in original_bundle.features if f.key == "FEATURE-002"), None)

        result = runner.invoke(
            app,
            [
                "plan",
                "update-feature",
                "--batch-updates",
                str(updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Verify partial updates
        with incomplete_plan.open() as f:
            updated_bundle_data = yaml.safe_load(f)
            updated_bundle = PlanBundle(**updated_bundle_data)

        updated_feature_1 = next((f for f in updated_bundle.features if f.key == "FEATURE-001"), None)
        updated_feature_2 = next((f for f in updated_bundle.features if f.key == "FEATURE-002"), None)

        assert updated_feature_1 is not None
        assert updated_feature_1.confidence == 0.95
        # Other fields should remain unchanged
        assert updated_feature_1.title == original_feature_1.title if original_feature_1 else True

        assert updated_feature_2 is not None
        assert updated_feature_2.title == "New Title"
        # Other fields should remain unchanged
        assert updated_feature_2.confidence == original_feature_2.confidence if original_feature_2 else True


class TestBatchStoryUpdates:
    """Test batch updates for stories via file upload."""

    def test_batch_update_stories_from_file(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test updating multiple stories via batch file."""
        monkeypatch.chdir(workspace)

        # Create batch update file
        updates_file = workspace / "story_updates.json"
        updates = [
            {
                "feature": "FEATURE-001",
                "key": "STORY-001",
                "title": "Updated Story 1",
                "acceptance": ["Given X, When Y, Then Z"],
                "story_points": 5,
                "value_points": 3,
                "confidence": 0.9,
            },
        ]
        updates_file.write_text(json.dumps(updates, indent=2))

        result = runner.invoke(
            app,
            [
                "plan",
                "update-story",
                "--batch-updates",
                str(updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Verify updates were applied
        with incomplete_plan.open() as f:
            updated_bundle_data = yaml.safe_load(f)
            updated_bundle = PlanBundle(**updated_bundle_data)

        # Find updated story
        feature_1 = next((f for f in updated_bundle.features if f.key == "FEATURE-001"), None)
        assert feature_1 is not None

        story_1 = next((s for s in feature_1.stories if s.key == "STORY-001"), None)
        assert story_1 is not None
        assert story_1.title == "Updated Story 1"
        assert story_1.acceptance == ["Given X, When Y, Then Z"]
        assert story_1.story_points == 5
        assert story_1.value_points == 3
        assert story_1.confidence == 0.9

    def test_batch_update_stories_multiple_features(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test batch updates for stories across multiple features."""
        monkeypatch.chdir(workspace)

        # Add a story to FEATURE-002 first
        with incomplete_plan.open() as f:
            bundle_data = yaml.safe_load(f)
            bundle = PlanBundle(**bundle_data)

        feature_2 = next((f for f in bundle.features if f.key == "FEATURE-002"), None)
        if feature_2:
            feature_2.stories.append(
                Story(
                    key="STORY-002",
                    title="Story 2",
                    acceptance=[],
                    story_points=0,
                    value_points=0,
                    confidence=0.5,
                    draft=False,
                    scenarios=None,
                    contracts=None,
                )
            )

            with incomplete_plan.open("w") as f:
                yaml.dump(bundle.model_dump(), f, default_flow_style=False)

        # Create batch update file for multiple stories
        updates_file = workspace / "multi_story_updates.json"
        updates = [
            {
                "feature": "FEATURE-001",
                "key": "STORY-001",
                "acceptance": ["Given Feature 1 Story, When executed, Then it works"],
                "confidence": 0.9,
            },
            {
                "feature": "FEATURE-002",
                "key": "STORY-002",
                "acceptance": ["Given Feature 2 Story, When executed, Then it works"],
                "confidence": 0.85,
            },
        ]
        updates_file.write_text(json.dumps(updates, indent=2))

        result = runner.invoke(
            app,
            [
                "plan",
                "update-story",
                "--batch-updates",
                str(updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert result.exit_code == 0

        # Verify both stories were updated
        with incomplete_plan.open() as f:
            updated_bundle_data = yaml.safe_load(f)
            updated_bundle = PlanBundle(**updated_bundle_data)

        feature_1 = next((f for f in updated_bundle.features if f.key == "FEATURE-001"), None)
        feature_2 = next((f for f in updated_bundle.features if f.key == "FEATURE-002"), None)

        assert feature_1 is not None
        story_1 = next((s for s in feature_1.stories if s.key == "STORY-001"), None)
        assert story_1 is not None
        assert any("Feature 1 Story" in acc for acc in story_1.acceptance)
        assert story_1.confidence == 0.9

        assert feature_2 is not None
        story_2 = next((s for s in feature_2.stories if s.key == "STORY-002"), None)
        assert story_2 is not None
        assert any("Feature 2 Story" in acc for acc in story_2.acceptance)
        assert story_2.confidence == 0.85


class TestInteractiveSelectiveUpdates:
    """Test interactive mode with selective updates via prompts."""

    def test_interactive_feature_update(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test interactive feature update with prompts."""
        monkeypatch.chdir(workspace)

        with (
            patch("specfact_cli.commands.plan.prompt_text") as mock_text,
            patch("specfact_cli.commands.plan.prompt_confirm") as mock_confirm,
        ):
            # Setup responses for interactive update
            mock_text.side_effect = [
                "Updated Interactive Title",  # title
                "Outcome 1, Outcome 2",  # outcomes
                "Acceptance 1, Acceptance 2",  # acceptance
            ]
            mock_confirm.side_effect = [
                True,  # Update title?
                True,  # Update outcomes?
                True,  # Update acceptance?
                False,  # Update constraints?
                False,  # Update confidence?
            ]

            result = runner.invoke(
                app,
                [
                    "plan",
                    "update-feature",
                    "--key",
                    "FEATURE-001",
                    "--plan",
                    str(incomplete_plan),
                ],
            )

            # Interactive mode may require more setup, but verify it doesn't crash
            assert result.exit_code in (0, 1)  # May exit with error if prompts not fully mocked

    def test_interactive_story_update(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test interactive story update with prompts."""
        monkeypatch.chdir(workspace)

        with (
            patch("specfact_cli.commands.plan.prompt_text") as mock_text,
            patch("specfact_cli.commands.plan.prompt_confirm") as mock_confirm,
        ):
            # Setup responses for interactive update
            mock_text.side_effect = [
                "Updated Story Title",  # title
                "Given X, When Y, Then Z",  # acceptance
            ]
            mock_confirm.side_effect = [
                True,  # Update title?
                True,  # Update acceptance?
                False,  # Update story points?
                False,  # Update value points?
                False,  # Update confidence?
            ]

            result = runner.invoke(
                app,
                [
                    "plan",
                    "update-story",
                    "--feature",
                    "FEATURE-001",
                    "--key",
                    "STORY-001",
                    "--plan",
                    str(incomplete_plan),
                ],
            )

            # Interactive mode may require more setup, but verify it doesn't crash
            assert result.exit_code in (0, 1)  # May exit with error if prompts not fully mocked


class TestCompleteBatchWorkflow:
    """Test complete workflow: list findings -> batch update features/stories."""

    def test_complete_batch_workflow(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test complete workflow: list findings -> batch update -> verify."""
        monkeypatch.chdir(workspace)

        # Step 1: List findings in JSON format
        list_result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--findings-format",
                "json",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert list_result.exit_code == 0

        # Parse findings
        output_lines = list_result.stdout.strip().split("\n")
        json_start = None
        for i, line in enumerate(output_lines):
            if line.strip().startswith("{"):
                json_start = i
                break

        if json_start is None:
            pytest.skip("No findings found")

        json_str = "\n".join(output_lines[json_start:])
        findings_data = json.loads(json_str)

        if len(findings_data["findings"]) == 0:
            pytest.skip("No findings to process")

        # Step 2: Create batch updates based on findings
        # For this test, we'll create updates for features
        updates_file = workspace / "workflow_updates.json"
        updates = [
            {
                "key": "FEATURE-001",
                "acceptance": ["Acceptance from workflow"],
                "confidence": 0.95,
            },
        ]
        updates_file.write_text(json.dumps(updates, indent=2))

        # Step 3: Apply batch updates
        update_result = runner.invoke(
            app,
            [
                "plan",
                "update-feature",
                "--batch-updates",
                str(updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert update_result.exit_code == 0

        # Step 4: Verify updates were applied
        with incomplete_plan.open() as f:
            updated_bundle_data = yaml.safe_load(f)
            updated_bundle = PlanBundle(**updated_bundle_data)

        feature_1 = next((f for f in updated_bundle.features if f.key == "FEATURE-001"), None)
        assert feature_1 is not None
        assert feature_1.acceptance == ["Acceptance from workflow"]
        assert feature_1.confidence == 0.95

    def test_copilot_llm_enrichment_workflow(self, workspace: Path, incomplete_plan: Path, monkeypatch):
        """Test Copilot LLM enrichment workflow: list findings -> LLM generates updates -> batch apply."""
        monkeypatch.chdir(workspace)

        # Step 1: List findings for LLM
        list_result = runner.invoke(
            app,
            [
                "plan",
                "review",
                "--list-findings",
                "--findings-format",
                "json",
                "--non-interactive",
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert list_result.exit_code == 0

        # Step 2: Simulate LLM generating batch updates (in real scenario, LLM would analyze findings)
        # LLM would generate comprehensive updates based on findings
        llm_updates_file = workspace / "llm_enrichment_updates.json"
        llm_updates = [
            {
                "key": "FEATURE-001",
                "title": "Enhanced Feature 1",
                "outcomes": ["Enhanced outcome 1", "Enhanced outcome 2"],
                "acceptance": [
                    "Given enhanced feature, When used, Then it works correctly",
                    "Given error case, When handled, Then error is reported",
                ],
                "confidence": 0.95,
            },
            {
                "feature": "FEATURE-001",
                "key": "STORY-001",
                "title": "Enhanced Story 1",
                "acceptance": ["Given story context, When executed, Then acceptance criteria met"],
                "story_points": 8,
                "value_points": 5,
                "confidence": 0.9,
            },
        ]
        llm_updates_file.write_text(json.dumps(llm_updates, indent=2))

        # Step 3: Apply feature updates
        feature_update_result = runner.invoke(
            app,
            [
                "plan",
                "update-feature",
                "--batch-updates",
                str(llm_updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert feature_update_result.exit_code == 0

        # Step 4: Apply story updates (separate file for stories)
        story_updates_file = workspace / "llm_story_updates.json"
        story_updates = [llm_updates[1]]  # Story update
        story_updates_file.write_text(json.dumps(story_updates, indent=2))

        story_update_result = runner.invoke(
            app,
            [
                "plan",
                "update-story",
                "--batch-updates",
                str(story_updates_file),
                "--plan",
                str(incomplete_plan),
            ],
        )

        assert story_update_result.exit_code == 0

        # Step 5: Verify all updates were applied
        with incomplete_plan.open() as f:
            updated_bundle_data = yaml.safe_load(f)
            updated_bundle = PlanBundle(**updated_bundle_data)

        feature_1 = next((f for f in updated_bundle.features if f.key == "FEATURE-001"), None)
        assert feature_1 is not None
        assert feature_1.title == "Enhanced Feature 1"
        assert len(feature_1.outcomes) == 2
        assert len(feature_1.acceptance) == 2
        assert feature_1.confidence == 0.95

        story_1 = next((s for s in feature_1.stories if s.key == "STORY-001"), None)
        assert story_1 is not None
        assert story_1.title == "Enhanced Story 1"
        assert any("story context" in acc for acc in story_1.acceptance)
        assert story_1.story_points == 8
        assert story_1.value_points == 5
        assert story_1.confidence == 0.9
