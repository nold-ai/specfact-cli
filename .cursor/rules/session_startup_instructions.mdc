---
description: This rule provides essential instructions for testing and building the project correctly, avoiding common pitfalls with test environment management.
globs:
alwaysApply: true
---

# Session Startup Instructions

## Core Workflow Reminders

- **Testing:** Run `hatch run smart-test`. Ensure total coverage is >= 80%.
- **Formatting:** Apply `black .`, `isort .`, `mypy .`, and `pylint src tests` before committing.
- **Commits:** Use the [Conventional Commits](https://www.conventionalcommits.org/) format.
- **Versioning:** When releasing, update `CHANGELOG.md`, `pyproject.toml`, `setup.py`, and `src/__init__.py`.
- **Docstrings:** Follow the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).

## Initial Context Checklist

1. **Review:
1.1. `GEMINI.md`: Check for any session-specific goals or instructions (applies to GEMINI CLI only).
1.2. `CLAUDE.md`: Check for any session-specific goals or instructions (applies to Claude CLI only).
2. `docs/README.md` to get the latest project status and priorities and see which plan is referenced as being worked on: Understand the current development phase and tasks based on the mentioned plan in the README.md file.
3. Outline your understanding of the current development phase and tasks based on the mentioned plan in the README.md file, before proceeding with any work. Ask the user for confirmation before proceeding.

## Documentation and Planning Guidelines

**CRITICAL**: When working with planning and documentation:

- **Work directly with major artifacts**: Update strategic plans, implementation plans, and analysis documents directly. Do NOT create plans for plans, tracking documents for tracking documents, or status artifacts for status artifacts.
- **Update existing artifacts**: Add status annotations (‚úÖ Complete, ‚è≥ In Progress, üü° Pending) directly to existing plan documents rather than creating separate status files.
- **Consolidate, don't multiply**: Only create new documentation artifacts when they add clear, unique value that cannot be captured in existing artifacts.
- **Performance metrics**: Record timing and performance data directly in implementation status documents, not in separate performance tracking files.
- **Test results**: Include test results and validation outcomes in the relevant implementation status or quality analysis documents.

**Examples of what NOT to do**:

- ‚ùå Creating `PHASE0_TRACKING.md` when `CODE2SPEC_STRATEGIC_PLAN.md` already exists
- ‚ùå Creating `STEP1_1_TEST_RESULTS.md` when `PHASE1_IMPLEMENTATION_STATUS.md` can be updated
- ‚ùå Creating `PERFORMANCE_METRICS.md` when performance data can go in implementation status

**Examples of what TO do**:

- ‚úÖ Update `CODE2SPEC_STRATEGIC_PLAN.md` with status annotations (‚úÖ Complete, ‚è≥ Next)
- ‚úÖ Add test results and performance metrics to `PHASE1_IMPLEMENTATION_STATUS.md`
- ‚úÖ Update `QUALITY_GAP_ANALYSIS.md` with measurement results and progress
